import numpy as np
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import (Input, Conv2D, AveragePooling2D,
                          concatenate, Dense, PReLU, Flatten)

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.initializers import Constant
from tensorflow.keras.metrics import Metric


class RedShiftClassificationModel(Model):
    def __init__(self,
                 input_img_shape,
                 redenning_shape=None,
                 num_redshift_classes=180):
        """A variant of an Inception model used by http://arxiv.org/abs/1806.06607

        Parameters
        ----------
        input_img_shape: tuple
            The shape of the input image
        redenning_shape : tuple
            The shape of the redenning array
        num_redshift_classes: int, default=180
            The number of redshift bins in the final classification task
        """
        # Input Layer Galactic Images
        image_input = Input(shape=input_img_shape)

        # redening information
        if redenning_shape:
            redening_input = Input(shape=redenning_shape)

        # Convolution Layer 1
        conv_1 = Conv2D(64,
                        kernel_size=(5, 5),
                        padding='same',
                        activation='linear',
                        kernel_initializer='glorot_uniform',
                        bias_initializer=Constant(0.1))
        conv_1_out = PReLU()(conv_1(image_input))

        # Pooling Layer 1
        pooling_layer1 = AveragePooling2D(pool_size=(2, 2),
                                          strides=2,
                                          padding='same')
        pooling_layer1_out = pooling_layer1(conv_1_out)

        # Inception Layer 1
        inception_layer1_out = self.add_inception_layer(pooling_layer1_out,
                                                        num_f1=48,
                                                        num_f2=64)

        # Inception Layer 2
        inception_layer2_out = self.add_inception_layer(inception_layer1_out,
                                                        num_f1=64,
                                                        num_f2=92)

        # Pooling Layer 2
        pooling_layer2 = AveragePooling2D(pool_size=(2, 2),
                                          strides=2,
                                          padding='same')
        pooling_layer2_out = pooling_layer2(inception_layer2_out)

        # Inception Layer 3
        inception_layer3_out = self.add_inception_layer(pooling_layer2_out, 92, 128)

        # Inception Layer 4
        inception_layer4_out = self.add_inception_layer(inception_layer3_out, 92, 128)

        # Pooling Layer 3
        pooling_layer3 = AveragePooling2D(pool_size=(2, 2),
                                          strides=2,
                                          padding='same')
        pooling_layer3_out = pooling_layer3(inception_layer4_out)

        # Inception Layer 5
        inception_layer5_out = self.add_inception_layer(pooling_layer3_out,
                                                        92, 128,
                                                        kernel_5=False)

        # input_to_pooling = cur_inception_in
        input_to_dense = Flatten(
                            data_format='channels_last')(inception_layer5_out)

        if redenning_shape:
            input_to_dense = concatenate([input_to_dense, redening_input], axis=1)

        dense1 = Dense(units=1096,
                       activation='linear',
                       kernel_initializer='glorot_uniform',
                       bias_initializer=Constant(0.1))

        dense1_out = PReLU()(dense1(input_to_dense))

        dense2 = Dense(units=1096,
                       activation='linear',
                       kernel_initializer='glorot_uniform',
                       bias_initializer=Constant(0.1))

        dense2_out = PReLU()(dense2(dense1_out))

        classifier_dense = Dense(units=num_redshift_classes,
                                 activation='softmax',
                                 kernel_initializer='glorot_uniform',
                                 bias_initializer=Constant(0.1))

        model_output = classifier_dense(dense2_out)
        if redenning_shape:
            super(RedShiftClassificationModel, self).__init__(
                inputs=[image_input, redening_input], outputs=model_output)
        else:
            super(RedShiftClassificationModel, self).__init__(
                inputs=image_input, outputs=model_output)
        # self.summary()

    def add_inception_layer(self,
                            input_weights,
                            num_f1,
                            num_f2,
                            kernel_5=True):
        """These convolutional layers take care of the inception layer"""
        # Conv Layer 1 and Layer 2: Feed them to convolution layers 5 and 6
        c1 = Conv2D(num_f1,
                    kernel_size=(1, 1),
                    padding='same',
                    activation='linear',
                    kernel_initializer='glorot_uniform',
                    bias_initializer=Constant(0.1))

        c1_out = PReLU()(c1(input_weights))
        if kernel_5:
            c2 = Conv2D(num_f1,
                        kernel_size=(1, 1),
                        padding='same',
                        activation='linear',
                        kernel_initializer='glorot_uniform',
                        bias_initializer=Constant(0.1))

            c2_out = PReLU()(c2(input_weights))

        # Conv Layer 3 : Feed to pooling layer 1
        c3 = Conv2D(num_f1,
                    kernel_size=(1, 1),
                    padding='same',
                    activation='linear',
                    kernel_initializer='glorot_uniform',
                    bias_initializer=Constant(0.1))

        c3_out = PReLU()(c3(input_weights))

        # Conv Layer 4: Feed directly to concat
        c4 = Conv2D(num_f2,
                    kernel_size=(1, 1),
                    padding='same',
                    activation='linear',
                    kernel_initializer='glorot_uniform',
                    bias_initializer=Constant(0.1))

        c4_out = PReLU()(c4(input_weights))

        # Conv Layer 5: Feed from c1, feed to concat
        c5 = Conv2D(num_f2,
                    kernel_size=(3, 3),
                    padding='same',
                    activation='linear',
                    kernel_initializer='glorot_uniform',
                    bias_initializer=Constant(0.1))

        c5_out = PReLU()(c5(c1_out))

        # Conv Layer 6: Feed from c2, feed to concat
        if kernel_5:
            c6 = Conv2D(num_f2,
                        kernel_size=(5, 5),
                        padding='same',
                        activation='linear',
                        kernel_initializer='glorot_uniform',
                        bias_initializer=Constant(0.1))

            c6_out = PReLU()(c6(c2_out))

        # Pooling Layer 1: Feed from conv3, feed to concat
        p1 = AveragePooling2D(pool_size=(2, 2), strides=1, padding='same')
        p1_out = p1(c3_out)

        if kernel_5:
            return concatenate([c4_out, c5_out, c6_out, p1_out])
        else:
            return concatenate([c4_out, c5_out, p1_out])

    def compile(self,
                optimizer=None,
                loss=None,
                metrics=None,
                loss_weights=None,
                sample_weight_mode=None,
                weighted_metrics=None,
                **kwargs):
        if optimizer is None:
            optimizer = Adam(lr=0.001)

        if loss is None:
            loss = 'sparse_categorical_crossentropy'

        if metrics is None:
            metrics = ['sparse_categorical_accuracy']

        super().compile(optimizer=optimizer,
                        loss=loss,
                        metrics=metrics,
                        loss_weights=loss_weights,
                        sample_weight_mode=sample_weight_mode,
                        weighted_metrics=weighted_metrics,
                        **kwargs
                        )


class RedshiftMetric(Metric):
    def __init__(self,
                 name='def_redshift_metric',
                 rs_num_bins=180,
                 rs_max_val=3.5,
                 **kwargs):
        super().__init__(name=name, **kwargs)
        self.rs_num_bins = rs_num_bins,
        self.rs_max_val = rs_max_val

    def residuals(self, y_true, y_pred):
        step = self.rs_max_val / self.rs_num_bins
        bins = np.arange(0, self.rs_max_val, step) + (step/2)
        tf.reduce_sum(tf.multiply(y_pred, bins), axis=1)

        return (y_pred - y_true) / (y_true + 1)


class PredictionBias(RedshiftMetric):
    def __init__(self, name='pred_bias', **kwargs):
        super().__init__(name=name, **kwargs)

    def update_state(self, y_true, y_pred, sample_weight=None):
        residuals = self.residuals(y_true, y_pred)
        self.value = tf.math.reduce_mean(residuals)


class DeviationMAD(RedshiftMetric):
    def __init__(self, name='MAD_dev', **kwargs):
        super().__init__(name=name, **kwargs)

    def update_state(self, y_true, y_pred, sample_weight=None):
        residuals = self.residuals(y_true, y_pred)
        res_med = tf.numpy_function(np.median, [residuals], residuals.dtype)

        self.value = tf.numpy_function(np.median, [tf.abs(residuals - res_med)], residuals.dtype) * 1.4826


def FractionOutliers(RedshiftMetric):
    pass


if __name__ == '__main__':
    from model_utils import plot_model

    model = RedShiftClassificationModel(
        (64, 64, 5),
        redenning_shape=(1,)
    )

    model.compile()

    plot_model(
        model,
        'model.png'
    )
